# Interview Trainer (Лабораторная работа 2)

`Interview Trainer` — мультиагентный тренажёр технического интервью в чат-формате.
Система генерирует вопросы под профиль кандидата, принимает ответы, оценивает их по рубрике, подсвечивает слабые темы и формирует рекомендации, а также сохраняет прогресс в долговременную память.

Ключевые технологии: `LangChain` (tools), `LangGraph` (workflow/состояние), OpenAI‑совместимый endpoint (например, vLLM).

## Возможности

- **Мультиагентные роли (узлы)**:
  - **`Router`**: выбор маршрута (`interview`, `evaluate_only`, `plan_only`, `summary`) и инициализация профиля.
  - **`Interviewer`**: выдаёт *один* вопрос за шаг, учитывая профиль и слабые темы; старается избегать повторов и чередовать форматы.
  - **`Evaluator`**: оценивает ответ по рубрике (score 0–100 и score по критериям), выделяет слабые темы и рекомендации; может вернуть уточняющий вопрос.
  - **`Coach`**: формирует план прокачки на 7–14 дней по результатам оценки.
  - **`Summary`**: строит итог **только по текущей сессии** (`qa_history`), без подтягивания прошлых сессий.

- **Tool calling (3 инструмента)**:
  - `session_timer_logger_tool` — лог событий/тайминг сессии;
  - `rubric_generator_tool` — рубрика оценивания под профиль;
  - `weak_topics_counter_tool` — счетчики слабых тем из результата оценки.

- **Память**:
  - **долговременная**: `data/memory.json` (профиль, статистика слабых тем, история сессий);
  - **сессионная**: `qa_history` (только Q/A текущего запуска для `Summary`);
  - **анти-повторы**: `asked_questions` (история уже выданных вопросов в текущей сессии, даже если пользователь нажимал `next` без ответа).

Подробности по архитектуре: `docs/architecture.md`.

## Быстрый старт

### Требования

- Python 3.11+
- Модель через OpenAI‑совместимый endpoint (например, Qwen через vLLM)

Переменные окружения:
- `LITELLM_URL_BASE`
- `LITELLM_API_KEY`
- `MODEL_NAME`
- (опционально) `INTERVIEW_MEMORY_PATH` (путь к файлу памяти вместо `data/memory.json`)

### Установка

```bash
pip install -e .
```

### Запуск через CLI

```bash
interview-trainer \
  --position "Python Backend Developer" \
  --stack "Python, FastAPI, PostgreSQL" \
  --level junior \
  --goals "Подготовиться к техническому интервью"
```

Альтернатива:

```bash
python -m interview_trainer \
  --position "Python Backend Developer" \
  --stack "Python, FastAPI, PostgreSQL" \
  --level junior \
  --goals "Подготовиться к техническому интервью"
```

Команды в чате:
- `next` — следующий вопрос;
- текстом — ответ на текущий вопрос;
- `summary` — итог по текущей сессии;
- `stop` — завершить интервью.

### Запуск из ноутбука

Откройте `interview_trainer_demo.ipynb` и выполните:
- ячейку создания `InterviewChatbot`;
- `chatbot.run_interactive()`.

## Структура проекта (основное)

- `src/interview_trainer/chatbot.py` — чат-цикл: `next` → вопрос → ответ → оценка; вывод в консоль/ноутбук.
- `src/interview_trainer/nodes.py` — реализация узлов `Router/Interviewer/Evaluator/Coach/Summary` + `memory_update_node`.
- `src/interview_trainer/graph.py` — сборка `StateGraph` (маршрутизация и retry‑цикл evaluator→interviewer).
- `src/interview_trainer/state.py` — `InterviewTrainerState` (TypedDict) со всеми полями состояния.
- `src/interview_trainer/prompts.py` — русские системные промпты.
- `src/interview_trainer/tools.py` — инструменты (tools).
- `data/memory.json` — долговременная память (создаётся/обновляется при работе).

##  Рефлексия по проекту

### Что получилось хорошо

- Получился **цельный, но небольшой** пример мультиагентной системы: маршрутизация, handoff между ролями, tools и память.
- В чат-формате удобно видно “конвейер”: вопрос → ответ → оценка → рекомендации → следующий вопрос/summary.
- Добавлены **анти-повторы** и **разнообразие типов** вопросов (conceptual/design/what_if/debugging/testing/performance/sql), включая короткие `code_snippet` для debugging/code‑review.

### Что не идеально (важное ограничение)

- **Оценки не всегда корректные/стабильные.** Это нормально для LLM‑оценивания: один и тот же ответ может получать разные баллы из‑за неоднозначности формулировки вопроса, недостатка контекста (предположения кандидата не озвучены), и “шумности” самой модели‑судьи.

### Что стоит улучшить дальше

- **Добавить уточняющие вопросы перед финальным score.** Сейчас `Evaluator` может вернуть `follow_up_question`, но он не встроен как обязательный шаг оценки. Улучшение: делать мини‑диалог “уточнение → дооценка” при низкой уверенности, либо когда ответ неполный/двусмысленный.
- **Сделать оценку более проверяемой**: просить `Evaluator` привязывать баллы к конкретным пунктам рубрики (“критерий → наблюдение → балл”), чтобы пользователь понимал причину оценок.
- **Стабилизировать скоринг**: самосогласование (несколько оценок → агрегирование), либо более строгая схема с ограниченными шкалами и примерами “что такое 40/70/90”.
- **Связать уточняющий вопрос с `next`**: если `Evaluator` вернул `follow_up_question`, предлагать его как следующий шаг, а не генерировать новый независимый вопрос.
